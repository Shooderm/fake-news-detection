{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktxgf9j9Qtsa"
   },
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4mB39YeH_hq9"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "NUM_TOPIC = 6\n",
    "# set the number of topics here (from 5 to 10)\n",
    "\n",
    "MIN_COUNTS = 5\n",
    "MAX_COUNTS = 150\n",
    "# words with count < MIN_COUNTS and count > MAX_COUNTS will be removed\n",
    "\n",
    "MIN_LENGTH = 15\n",
    "# minimum document length (number of words)after preprocessing\n",
    "\n",
    "# half the size of the context around a word\n",
    "HALF_WINDOW_SIZE = 5\n",
    "# it must be that 2*HALF_WINDOW_SIZE < MIN_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JGKbfuNRjQc"
   },
   "source": [
    "# Load NLP model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FR34u4nP_YXl",
    "outputId": "861ed214-5946-469f-9062-a0def4683795",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBmEPjDP_pA4"
   },
   "outputs": [],
   "source": [
    "# Load NLP model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Load dataset\n",
    "docs = [(i, doc) for i, doc in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0ytOxCCAty2"
   },
   "source": [
    "# Preprocess dataset and create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "Ki7GjuhjQ7n9",
    "outputId": "33a12b80-4ec0-4084-8e17-40718666f386"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [00:22<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed short documents: 3\n",
      "total number of tokens: 58294\n",
      "number of tokens to be removed: 14897\n",
      "number of additionally removed short documents: 1\n",
      "total number of tokens: 43384\n",
      "\n",
      "minimum word count number: 5\n",
      "this number can be less than MIN_COUNTS because of document removal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess dataset\n",
    "encoded_docs, decoder, word_counts = preprocess(docs, nlp, MIN_LENGTH, MIN_COUNTS, MAX_COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLv4ADMLAQAh"
   },
   "outputs": [],
   "source": [
    "# new ids will be created for the documents.\n",
    "# create a way of restoring initial ids:\n",
    "doc_decoder = {i: doc_id for i, (doc_id, doc) in enumerate(encoded_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "INOBDxuaAQ8R",
    "outputId": "555c0569-66c9-4035-9d11-81ae1e0d910e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [00:00, 4403.78it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "# new ids are created here\n",
    "for index, (_, doc) in tqdm(enumerate(encoded_docs)):\n",
    "    windows = get_windows(doc, HALF_WINDOW_SIZE)\n",
    "    # index represents id of a document, \n",
    "    # windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    data += [[index, w[0]] + w[1] for w in windows]\n",
    "\n",
    "data = np.array(data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9AnFFeNHATe1",
    "outputId": "ef30be14-7a20-49b0-c911-9a08a7a308c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 492,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a row in 'data' contains:\n",
    "# id of a document, id of a word in this document, a window around this word\n",
    "# 1 + 1 + 10\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PX40mfdxAW4g",
    "outputId": "d4e572c9-4065-4b80-ae69-03cf3149ec1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43384"
      ]
     },
     "execution_count": 493,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of windows (equals to the total number of tokens)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90-RoNHEAv95"
   },
   "source": [
    "# Get unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6_yD6-jAwR0"
   },
   "outputs": [],
   "source": [
    "word_counts = np.array(word_counts)\n",
    "unigram_distribution = word_counts/sum(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zh-dF37FBJe2"
   },
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "To2Qxbc6BJ9u",
    "outputId": "29768a55-330f-4ccd-d770-81f7d484faf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "CPU times: user 1min 29s, sys: 117 ms, total: 1min 29s\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_size = len(decoder)\n",
    "embedding_dim = 50\n",
    "\n",
    "# train a skip-gram word2vec model\n",
    "texts = [[str(j) for j in doc] for i, doc in encoded_docs]\n",
    "print(len(texts))\n",
    "model = models.Word2Vec(texts, size=embedding_dim, window=5, workers=4, min_count=1, sg=1, negative=15, iter=70)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_vectors = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
    "for i in decoder:\n",
    "    word_vectors[i] = model.wv[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kdpsK0Sj6eee",
    "outputId": "3e04e61b-eac2-47f8-895c-678e66dc51b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362"
      ]
     },
     "execution_count": 496,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XctQ2l9pBOq7"
   },
   "source": [
    "# Prepare initialization for document weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YEkLdZmBVbW"
   },
   "outputs": [],
   "source": [
    "texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ag20o9PTBX9f",
    "outputId": "05d9e702-f962-4eb7-9fcb-930884db99e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 476 ms, sys: 2.02 ms, total: 478 ms\n",
      "Wall time: 478 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = NUM_TOPIC\n",
    "lda = models.LdaModel(corpus, alpha=0.9, id2word=dictionary, num_topics=n_topics)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "0J9ikhrHTQJF",
    "outputId": "42101b20-9dfc-476c-8b20-8d8bd3c64502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1 : law right news look thing republican need public war america\n",
      "topic 2 : get justice work take candidate presidential america ask republican house\n",
      "topic 3 : million police pay presidential american story take call house find\n",
      "topic 4 : house vote korea police news thing million north republican try\n",
      "topic 5 : vote early story way family city voter police work america\n",
      "topic 6 : life black take ask find work americans family right thing\n"
     ]
    }
   ],
   "source": [
    "for i, topics in lda.show_topics(n_topics, formatted=False):\n",
    "    print('topic', i + 1, ':', ' '.join([t for t, _ in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Id4TLUm3Ber5",
    "outputId": "b89942ca-65a5-4c10-dac9-96c31381c03a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 1088.17it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_weights_init = np.zeros((len(corpus_lda), n_topics))\n",
    "for i in tqdm(range(len(corpus_lda))):\n",
    "    topics = corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTYnRa2VBiTr"
   },
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMrjqFHaBflk"
   },
   "outputs": [],
   "source": [
    "np.save('data.npy', data)\n",
    "np.save('word_vectors.npy', word_vectors)\n",
    "np.save('unigram_distribution.npy', unigram_distribution)\n",
    "np.save('decoder.npy', decoder)\n",
    "np.save('doc_decoder.npy', doc_decoder)\n",
    "np.save('doc_weights_init.npy', doc_weights_init)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KFq4vjeJvtJm",
    "iuaoty5lxHiF",
    "ZsQK8qWN8fUO",
    "6_Q8L1QS3F8t",
    "IfOfdLGh3LPg",
    "eAUGWnF13S29",
    "8opO45TY3e6N",
    "lC4fWIJN3pDk"
   ],
   "name": "get_windows.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
